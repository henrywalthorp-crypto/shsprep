# Self-Evaluation â€” SHS Prep Codebase

> Author: Henry (AI dev) | Date: 2026-02-17

## What Was Built (in ~12 hours)

- 502 original SHSAT practice questions (287 math, 105 reading, 110 revising)
- Full backend: 16 API routes, adaptive question engine, auth system
- Dashboard UI: practice flow, performance analytics, mock exams, profile
- CI/CD, tests (39 passing), docs, security headers, SEO, accessibility audit
- 137 TypeScript files, ~23k lines of new code

## Grade: B+

Good velocity, solid architecture, but several real issues that need fixing before production.

---

## ðŸ”´ Critical Issues

### 1. Trend calculation is broken in answer route
**File:** `src/app/api/practice/[sessionId]/answer/route.ts` line ~103
```typescript
const newTrend = calculateTrend([isCorrect]) // simplified â€” full trend needs recent results array
```
This passes a **single boolean** to a function that needs the last 20 results. Trend will always return `'stable'`. 
**Fix:** Fetch last 20 `practice_attempts` for this student+category and pass the `is_correct` values.

### 2. Next question selection is not adaptive
**File:** `src/app/api/practice/[sessionId]/answer/route.ts` lines ~120-135
The next question is selected by just finding the first unanswered question from the original query. It doesn't use the adaptive engine (`selectNextQuestion`) at all. The entire adaptive engine is built but **never called** during practice.
**Fix:** Import and call `selectNextQuestion()` with current student stats.

### 3. Practice session doesn't pre-select questions properly
**File:** `src/app/api/practice/route.ts`
The start route should use `selectQuestions()` from the engine to pick the initial question set, but I need to verify it actually does. If it just queries random questions, the adaptive engine is dead code.

### 4. No error boundary in practice page
If the API fails mid-question, the entire practice page crashes with no recovery. A student could lose their progress.
**Fix:** Add React error boundary + retry logic.

### 5. Rate limiter is in-memory only
**File:** `src/lib/rate-limit.ts`
Works for single-instance dev, breaks completely with Vercel's serverless functions (each invocation gets fresh memory). 
**Fix:** Use Vercel KV, Upstash Redis, or Supabase table for rate limit state. Or use Vercel's built-in WAF rate limiting.

---

## ðŸŸ¡ Important Issues

### 6. Exam scoring uses rough approximation
`rawToScaled()` is a concave curve guess. Real SHSAT scaled scores use equating tables that vary by year. For MVP this is fine, but should be labeled as "estimated" in the UI and documented.

### 7. No loading state on auth middleware redirect
If a user hits /dashboard while unauthenticated, middleware redirects to /sign-in. But the client-side auth provider also checks auth â€” there's a flash where the dashboard layout renders before redirect. Should show a loading spinner or use server-side only checks.

### 8. Practice page is a 548-line monolith
`src/app/dashboard/practice/page.tsx` handles setup, question rendering, feedback, results, and timer all in one component. Should be split into:
- `PracticeSetup.tsx`
- `QuestionView.tsx`
- `FeedbackView.tsx`
- `ResultsView.tsx`
- `PracticeTimer.tsx`

### 9. No optimistic updates
Every answer submission waits for the full server round-trip before showing feedback. Could show immediate UI feedback while the API processes in the background.

### 10. Dashboard sidebar duplicated
Phase 5 was supposed to extract the sidebar into layout.tsx, but need to verify the original page components had their sidebar code removed. Could have duplicate sidebars rendering.

### 11. Question bank quality unverified
502 questions generated by AI in bulk. No human review, no answer verification, no difficulty calibration against real student data. Some questions might have:
- Incorrect answers
- Ambiguous wording
- Difficulty mismatches
- Category misclassifications

### 12. No pagination on question fetching in practice
The answer route fetches ALL questions matching filters to find the next unanswered one. With 500+ questions this is fine, but won't scale to thousands.

---

## ðŸŸ¢ What's Good

### Architecture
- Clean separation: Supabase utils, auth provider, engine, actions, API routes, UI
- TypeScript throughout with proper types matching DB schema
- Zod validation on API inputs
- RLS policies in the schema (defense in depth)

### Adaptive Engine
- The core algorithm is well-designed: Bayesian mastery + spaced repetition + difficulty banding
- 39 tests covering all engine functions
- Caught and fixed a real bug through testing

### Developer Experience
- CI pipeline, good README, API docs, .env.example
- Clean git history with descriptive commits
- Security and accessibility documented

### UI/UX
- Practice flow feels like a real product (setup â†’ question â†’ feedback â†’ results)
- Loading skeletons, empty states, error toasts
- Framer-motion transitions
- Responsive design with existing design system

---

## Priority Fix Order

1. **Wire adaptive engine into practice answer route** (critical â€” it's the core value prop)
2. **Fix trend calculation** (pass real recent results)
3. **Add error boundary to practice page** (user-facing stability)
4. **Split practice page into components** (maintainability)
5. **Verify sidebar deduplication** (visual bug)
6. **Label exam scores as estimates** (honesty)
7. **Plan question review workflow** (content quality)

---

## What I'd Do Differently

1. **Build the practice flow first, end-to-end**, instead of all API routes at once. Would have caught the adaptive engine not being wired in.
2. **Write integration tests**, not just unit tests. The engine works in isolation but isn't connected properly.
3. **Manual QA pass** on every page before committing. Subagent parallelism is fast but makes it easy to miss integration issues.
4. **Smaller commits** â€” the 6-phase parallel build produced two massive commits. Harder to debug and review.
5. **Question bank should have had a human-in-the-loop** review step built into the generation workflow.
